{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_coordinate(four_coords, divisions_longs, devision_lats, if_big_box):\n",
    "    \n",
    "    if if_big_box:\n",
    "        [min_latitude, max_latitude, min_longitude, max_longitude] = [float(x) for x in four_coords.split(':')]\n",
    "    else:\n",
    "         [min_latitude, max_latitude, min_longitude, max_longitude] = four_coords\n",
    "\n",
    "    longitude_step = (max_longitude - min_longitude) / divisions_longs\n",
    "    latitude_step = (max_latitude - min_latitude) / devision_lats  # Typo: should be \"divisions_lats\"\n",
    "\n",
    "    coord_boxes = []\n",
    "    \n",
    "    # Generate bounding boxes for each grid cell\n",
    "    for i in range(divisions_longs):\n",
    "        for j in range(devision_lats):\n",
    "            box_min_lat = round(min_latitude + j * latitude_step, 5)\n",
    "            box_max_lat = round(min_latitude + (j + 1) * latitude_step, 5)\n",
    "            box_min_lon = round(min_longitude + i * longitude_step, 5)\n",
    "            box_max_lon = round(min_longitude + (i + 1) * longitude_step, 5)\n",
    "\n",
    "            # Store bounding box as a string in the format \"min_lat:max_lat:min_lon:max_lon\"\n",
    "            box_str = f\"{box_min_lat}:{box_max_lat}:{box_min_lon}:{box_max_lon}\"\n",
    "            coord_boxes.append(box_str)\n",
    "    \n",
    "    return coord_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vancouver_grid(head, divisions_longs, devision_lats):\n",
    "    \"\"\"\n",
    "    Generates a grid of latitude-longitude bounding boxes within Vancouver's city boundary.\n",
    "\n",
    "    Parameters:\n",
    "    head (dict): Headers for the API request.\n",
    "    divisions_longs (int): Number of divisions along the longitude (default is 15).\n",
    "    devision_lats (int): Number of divisions along the latitude (default is 15).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings representing bounding boxes in the format \"min_lat:max_lat:min_lon:max_lon\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # API endpoint for Vancouver city boundary geo-coordinates\n",
    "    van_geo_info_url = 'https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/city-boundary/records?limit=20'\n",
    "    \n",
    "    # Fetch geographical data from the API\n",
    "    response = requests.get(van_geo_info_url, headers=head)\n",
    "    geo_data = response.json()\n",
    "    \n",
    "    # Extract the city boundary coordinates\n",
    "    contour = geo_data['results'][0]['geom']['geometry']['coordinates']\n",
    "\n",
    "    # Flatten the list of coordinates and extract longitude and latitude values separately\n",
    "    longitudes = [coord[0] for sublist in contour for coord in sublist]\n",
    "    latitudes = [coord[1] for sublist in contour for coord in sublist]\n",
    "\n",
    "    # Determine the minimum and maximum longitude and latitude values\n",
    "    max_longitude = max(longitudes)\n",
    "    min_longitude = min(longitudes)\n",
    "    max_latitude = max(latitudes)\n",
    "    min_latitude = min(latitudes)\n",
    "    four_coords = [min_latitude, max_latitude, min_longitude, max_longitude]\n",
    "\n",
    "    \n",
    "    coord_boxes = split_coordinate(four_coords, divisions_longs, devision_lats, if_big_box = 0)\n",
    " \n",
    "    \n",
    "    return coord_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing_count(head, coord_box):\n",
    "    \"\"\"\n",
    "    Fetches the number of real estate listings within a specified coordinate box from Redfin.\n",
    "\n",
    "    Parameters:\n",
    "    head (dict): Headers for the HTTP request.\n",
    "    coord_box (str): A string representing the bounding box in the format \"min_lat:max_lat:min_lon:max_lon\".\n",
    "\n",
    "    Returns:\n",
    "    tuple: (viewport_url, select_listing_count, total_listing_count)\n",
    "        - viewport_url (str): The URL used for the request.\n",
    "        - select_listing_count (int): The number of listings shown in the current viewport.\n",
    "        - total_listing_count (int): The total number of listings available.\n",
    "        - If no listings are found, returns 'no_listing'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the given coordinate box\n",
    "    viewport_url = f\"https://www.redfin.ca/bc/vancouver/filter/viewport={coord_box}\"\n",
    "    \n",
    "    # Send a GET request to fetch the webpage\n",
    "    resp = requests.get(viewport_url, headers=head)\n",
    "\n",
    "    # Raise an error if the request fails (non-200 status code)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Failing in webpage requests\")\n",
    "    \n",
    "    # Parse the HTML response using BeautifulSoup\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # Check if the page indicates no listings are available\n",
    "    if soup.find('div', {'class': 'HomeViews reversePosition'}).find('h2'):\n",
    "        return 'no_listing'\n",
    "    \n",
    "    # Extract the listing summary section\n",
    "    listing_summary = soup.find('div', {'class': \"homes summary reversePosition\"})\n",
    "\n",
    "    # Use regex to extract numeric values from the listing summary\n",
    "    select_listing_count, total_listing_count = re.findall(r'\\d{1,10}(?:,\\d{1,10})*', listing_summary.text)\n",
    "    \n",
    "    # Convert extracted strings into integers, handling comma formatting\n",
    "    select_listing_count, total_listing_count = int(select_listing_count), int(total_listing_count.replace(',', ''))\n",
    "    \n",
    "    return viewport_url, select_listing_count, total_listing_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_redfin(head, viewport_url, page):\n",
    "    \"\"\"\n",
    "    Crawls a specific page of real estate listings from Redfin within a given viewport.\n",
    "\n",
    "    Parameters:\n",
    "    head (dict): Headers for the HTTP request.\n",
    "    viewport_url (str): Base URL for the listings search.\n",
    "    page (int): Page number to crawl.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of BeautifulSoup objects representing individual property listings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the specified page number\n",
    "    target_url = f\"{viewport_url}/page-{page}\"\n",
    "    \n",
    "    # Send a GET request to fetch the webpage\n",
    "    resp = requests.get(target_url, headers=head)\n",
    "\n",
    "    # Raise an error if the request fails (non-200 status code)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Failing in webpage requests\")\n",
    "    \n",
    "    # Parse the HTML response using BeautifulSoup\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "header =  {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"}\n",
    "geo_grid = vancouver_grid(head = header, divisions_longs=6, devision_lats=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.redfin.ca/bc/vancouver/filter/viewport=['49.19835:49.21799:-123.22479:-123.19118', '49.21799:49.23762:-123.22479:-123.19118', '49.23762:49.25726:-123.22479:-123.19118', '49.25726:49.2769:-123.22479:-123.19118', '49.2769:49.29654:-123.22479:-123.19118', '49.29654:49.31617:-123.22479:-123.19118', '49.19835:49.21799:-123.19118:-123.15758', '49.21799:49.23762:-123.19118:-123.15758', '49.23762:49.25726:-123.19118:-123.15758', '49.25726:49.2769:-123.19118:-123.15758', '49.2769:49.29654:-123.19118:-123.15758', '49.29654:49.31617:-123.19118:-123.15758', '49.19835:49.21799:-123.15758:-123.12397', '49.21799:49.23762:-123.15758:-123.12397', '49.23762:49.25726:-123.15758:-123.12397', '49.25726:49.2769:-123.15758:-123.12397', '49.2769:49.29654:-123.15758:-123.12397', '49.29654:49.31617:-123.15758:-123.12397', '49.19835:49.21799:-123.12397:-123.09036', '49.21799:49.23762:-123.12397:-123.09036', '49.23762:49.25726:-123.12397:-123.09036', '49.25726:49.2769:-123.12397:-123.09036', '49.2769:49.29654:-123.12397:-123.09036', '49.29654:49.31617:-123.12397:-123.09036', '49.19835:49.21799:-123.09036:-123.05676', '49.21799:49.23762:-123.09036:-123.05676', '49.23762:49.25726:-123.09036:-123.05676', '49.25726:49.2769:-123.09036:-123.05676', '49.2769:49.29654:-123.09036:-123.05676', '49.29654:49.31617:-123.09036:-123.05676', '49.19835:49.21799:-123.05676:-123.02315', '49.21799:49.23762:-123.05676:-123.02315', '49.23762:49.25726:-123.05676:-123.02315', '49.25726:49.2769:-123.05676:-123.02315', '49.2769:49.29654:-123.05676:-123.02315', '49.29654:49.31617:-123.05676:-123.02315'] 350 4645\n"
     ]
    }
   ],
   "source": [
    "viewport_url, select_listing_count, total_listing_count = listing_count(head=header, coord_box=geo_grid)\n",
    "print(viewport_url, select_listing_count, total_listing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 69\n"
     ]
    }
   ],
   "source": [
    "soup1 = crawling_redfin(head=header, viewport_url=viewport_url, page=1)\n",
    "info1 = soup1.find_all('script', {'type': 'application/ld+json'})\n",
    "info1 =  [json.loads(i.string) for i in info1]\n",
    "time.sleep(1)\n",
    "soup2 = crawling_redfin(head=header, viewport_url=viewport_url, page=2)\n",
    "info2 = soup2.find_all('script', {'type': 'application/ld+json'})\n",
    "info2 =  [json.loads(i.string) for i in info2]\n",
    "print(len(info1), len(info2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_extraction(result, result_event, result_event_list, further_invest, soup):\n",
    "    \n",
    "    info = soup.find_all('script', {'type':'application/ld+json'})\n",
    "    info =  [json.loads(i.string) for i in info]\n",
    "\n",
    "    for j, i in enumerate(info):\n",
    "        if isinstance(i, dict):\n",
    "            \n",
    "            type_i = i.get('@type')\n",
    "            \n",
    "            if type_i == 'Organization' or type_i == 'BreadcrumbList':\n",
    "                continue\n",
    "            \n",
    "            elif type_i == 'Event':\n",
    "                \n",
    "                location = i.get('location')\n",
    "            \n",
    "                if isinstance(location, list):\n",
    "                    try:\n",
    "                        address = i.get('location')[1].get('address').get('streetAddress')\n",
    "                        postalCode = i.get('location')[1].get('address').get('postalCode')\n",
    "                        latitude = i.get('location')[1].get('geo').get('latitude')\n",
    "                        longitude = i.get('location')[1].get('geo').get('longitude')\n",
    "                        url = i.get('url')\n",
    "\n",
    "                        result_event_list['address'].append(address)\n",
    "                        result_event_list['postalCode'].append(postalCode)\n",
    "                        result_event_list['latitude'].append(latitude)\n",
    "                        result_event_list['longitude'].append(longitude)\n",
    "                        result_event_list['url'].append(url)\n",
    "                    except:\n",
    "                        further_invest.append((j, i))\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        address = i.get('location').get('name')\n",
    "                        postalCode = i.get('location').get('address').get('postalCode') \n",
    "                        latitude = i.get('location').get('geo').get('latitude')\n",
    "                        longitude = i.get('location').get('geo').get('longitude')\n",
    "                        price = i.get('offers').get('price')\n",
    "                        url = i.get('url')\n",
    "                        \n",
    "                        result_event['address'].append(address)\n",
    "                        result_event['postalCode'].append(postalCode)\n",
    "                        result_event['latitude'].append(latitude)\n",
    "                        result_event['longitude'].append(longitude)\n",
    "                        result_event['price'].append(price)\n",
    "                        result_event['url'].append(url)\n",
    "                \n",
    "                        \n",
    "                    except:\n",
    "                        further_invest.append(i)\n",
    "\n",
    "\n",
    "        elif isinstance(i, list):\n",
    "            try: \n",
    "                i_1 = i[0]\n",
    "                address = i_1.get('address').get('streetAddress')\n",
    "                postalCode = i_1.get('address').get('postalCode')\n",
    "                latitude = i_1.get('geo').get('latitude')\n",
    "                longitude = i_1.get('geo').get('latitude')\n",
    "                sqr_footage = i_1.get('floorSize').get('value')\n",
    "                bedrooms = i_1.get('numberOfRooms')\n",
    "                url = i_1.get('url')\n",
    "                \n",
    "                i_2 = i[1]\n",
    "                price = i_2.get('offers').get('price')\n",
    "\n",
    "                result['address'].append(address)\n",
    "                result['postalCode'].append(postalCode)\n",
    "                result['latitude'].append(latitude)\n",
    "                result['longitude'].append(longitude)\n",
    "                result['price'].append(price)\n",
    "                result['square_footage'].append(sqr_footage)\n",
    "                result['bedroom'].append(bedrooms)\n",
    "                result['url'].append(url)\n",
    "            \n",
    "            except:\n",
    "                further_invest.append((j,i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_pages(total_count, items_per_page):\n",
    "    \"\"\"\n",
    "    Calculates the minimum number of pages required to display all items.\n",
    "\n",
    "    Parameters:\n",
    "    total_count (int): The total number of items to be displayed.\n",
    "    items_per_page (int): The maximum number of items that can be displayed per page.\n",
    "\n",
    "    Returns:\n",
    "    int: The minimum number of pages required.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use integer division to determine the number of pages needed\n",
    "    # Adding (items_per_page - 1) ensures proper rounding up\n",
    "    return (total_count + items_per_page - 1) // items_per_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3954926735.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef extracting_by_batch_method2(head, batch_num, divisions_longs=15, devision_lats=15, splitted_big_box = 0)\u001b[39m\n                                                                                                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def extracting_by_batch_method2(head, batch_num, divisions_longs=15, devision_lats=15, splitted_big_box = 0):   \n",
    "    \n",
    "    big_coord_boxes = []  \n",
    "    result_event = defaultdict(list)\n",
    "    result_event_list = defaultdict(list)\n",
    "    result = defaultdict(list)\n",
    "    url_with_issue = []\n",
    "\n",
    "    \n",
    "    if splitted_big_box:\n",
    "        coord_boxes = splitted_big_box\n",
    "\n",
    "    else: coord_boxes = vancouver_grid(head, divisions_longs, devision_lats)\n",
    "    coord_box_batch = np.array_split(coord_boxes, batch_num)\n",
    "    \n",
    "    for i in range(len(coord_box_batch)):\n",
    "        batch = coord_box_batch[i]\n",
    "        for coord_box in batch:\n",
    "            listing_info = listing_count(head, coord_box)\n",
    "            time.sleep(1)  \n",
    "            if listing_info == 'no_listing':\n",
    "                print(f\"Batch {i}-{coord_box} has no listings.\")\n",
    "                continue\n",
    "            else:\n",
    "                viewport_url, select_listing_count, total_listing_count = listing_info\n",
    "                if select_listing_count != total_listing_count:\n",
    "                    big_coord_boxes.append(coord_box)\n",
    "                    continue            \n",
    "                else:\n",
    "                    # Calculate the number of pages to crawl based on listings per page (assumed 9 per page)\n",
    "                    max_page = calculate_min_pages(select_listing_count, items_per_page=9)\n",
    "                    for page in range(1, max_page):\n",
    "                        soup = crawling_redfin(head, viewport_url, page)                        \n",
    "                        metrics_extraction(result, result_event, result_event_list, url_with_issue, soup)\n",
    "    \n",
    "    return result, result_event, result_event_list, big_coord_boxes, url_with_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
